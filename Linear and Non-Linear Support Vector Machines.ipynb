{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A Support Vector Machine / Network is a discrimininative classifier formally defined by a seperating hyperplane. Given a labeled training dataset, the algorithm outputs an optimal hyperplane which categorizes new examples. Support Vectors try to fit the largest possible margin between the support vectors**"
   ]
  },
  {
   "attachments": {
    "download.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![download.png](attachment:download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft Margin Classification** - *is a balance between keeping the margin as large as possible whilst limiting the margin violations. Unlike Logistic Regression Classifiers, SVM classifiers do not output probabilities for each class* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "#importing important libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as  sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import csv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datafile upload and data preparation\n",
    "df = pd.read_csv(\"diffreport2.csv\", sep= \",\")\n",
    "\n",
    "d1 = df.drop(\"name\", axis = 1)\n",
    "d2 = d1.drop(\"isotopes\", axis = 1)\n",
    "d3 = d2.drop(\"adduct\", axis = 1)\n",
    "d4 = d3.drop(\"tstat\", axis = 1)\n",
    "d5 = d4.drop(\"pvalue\", axis = 1)\n",
    "d6 = d5.drop(\"fold\", axis = 1)\n",
    "d7 = d6.drop(d6.columns[0], axis = 1)\n",
    "d8 = d7.drop(\"npeaks\", axis = 1)\n",
    "d9 = d8.drop(\"Eta6\", axis = 1)\n",
    "d10 = d9.drop(\"Eta8\", axis = 1)\n",
    "columns = ['Eta6_0', 'Eta6_2', 'Eta6_3', 'Eta8.1', 'Eta82', 'Eta83', 'ID']\n",
    "df1 = pd.DataFrame(d10, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creation of train and testing sets\n",
    "\n",
    "def get_train_test(df, y_col, x_cols, ratio):\n",
    "\n",
    "    mask = np.random.rand(len(df)) > ratio\n",
    "    df_train = df[mask]\n",
    "    df_test = df[~mask]\n",
    "       \n",
    "    Y_train = df_train[y_col].values\n",
    "    Y_test = df_test[y_col].values\n",
    "    X_train = df_train[x_cols].values\n",
    "    X_test = df_test[x_cols].values\n",
    "    return df_train, df_test, X_train, Y_train, X_test, Y_test\n",
    " \n",
    "y_col = 'ID'\n",
    "x_cols = list(df1.columns.values)\n",
    "x_cols.remove(y_col)\n",
    " \n",
    "train_test_ratio = 0.7\n",
    "df_train, df_test, X_train, Y_train, X_test, Y_test = get_train_test(df1, y_col, x_cols, train_test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_scv', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_clf = Pipeline(((\"scaler\", StandardScaler()), \n",
    "                   (\"linear_scv\", LinearSVC(C = 1, loss = \"hinge\")),\n",
    "                   ))\n",
    "svm_clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Support Vector Machine Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Feature Addition** -*one approach to handling nonlinear datasets is to add more features and this will result in  linearly seperable dataset. At low polynomial degree, implementation of polynomial features fails to handle complex datasets and with large polynomial degree, it creates a high number of features that make the model too slow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_svm_clf = Pipeline((\n",
    "                            (\"poly_features\", PolynomialFeatures(degree = 3)),\n",
    "                            (\"scaler\", StandardScaler()),\n",
    "                             (\"svm_clf\", LinearSVC(C = 10, loss = \"hinge\"))\n",
    "))\n",
    "\n",
    "polynomial_svm_clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Trick** - *is a method that avoids the explicit mapping that is needed to get linear learning algorithms to learn a nonlinear function or decision boundary, making it possible to get the same result as if one had added polynomial features.There is no combinatorial explosion of features since one does not add any features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    " (\"svm_clf\", SVC(kernel = \"poly\", degree = 3, coef0=1, C = 5))\n",
    "))\n",
    "poly_kernel_svm_clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity Feature** - *is based on a similarity function and it measures how much each instance resembles a particular landmark* *Defined below is the* **Gaussian Radial Bias Function**\n",
    "\n",
    "$$\\phi\\gamma(x, l) = exp(-\\gamma\\lvert\\lvert x-l \\rvert \\rvert ^2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=5, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline((\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel = \"rbf\", gamma = 5, C = 0.001)) # gamma is the regularization hyperparameter\n",
    "))\n",
    "rbf_kernel_svm_clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Regression** - *in this instance, the objective is reversed, instead of trying to fit the largest possible margin between two classes whilst limiting margin violations, the method tries to fit as many instances as possible in the margin whilst limiting margin  violations(instances that are not in the margin)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg= LinearSVR(epsilon = 1.5)\n",
    "svm_reg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Function and Predictions** *Linear SVM classifier model predicts the class of a new instance x by simply computing the decision function as shown*\n",
    "\n",
    "$$ \\hat{y} = \\begin {cases} 0, & \\text {if}\\ w^T * x + b < 0 \\\\ 1, & \\text {if} \\  w^T* x + b >= 0 \\end {cases}$$\n",
    "\n",
    "*The hard margin linear SVM Classifier objective can be expressed as a constrained optimization problem*\n",
    "\n",
    "*minimize *                    $$ \\frac{1}{2}w ^ T * w $$\n",
    "\n",
    "*The soft margin linear SVM Classifier objective is given as*\n",
    "*minimize*                     $$ \\frac{1}{2}w^T * w + C\\sum_{i = 1} ^{m}\\zeta ^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints(Quadratic Programming Problems). The general quadratic programming problem is given by *\n",
    "\n",
    "$$ \\frac{1}{2}p^ T*H* p + f^T * p $$\n",
    "\n",
    "Given a constrained optimization problem, known as the **primal problem**, it is possible to express a different but closely related problem, called its **dual problem**, which under some conditions, can have the same solutions or the upper bounds of the solution as the primal problem.\n",
    "*Dual solution to the primal solution*\n",
    "\n",
    "$$ \\hat{w} = \\sum_{i = 1}^{m}  \\hat{ \\alpha} t ^{(i)} x^{(i)} $$\n",
    "\n",
    "*The dual problem is faster is to solve than the primal when the number of training instances is smaller than the number of features*\n",
    "\n",
    "List of commonly used kernels\n",
    "\n",
    "** Linear Kernel**\n",
    "**Polynomial Kernel**\n",
    "**Gaussian RBF**\n",
    "**Sigmoid**\n",
    "\n",
    "**Mercer's Theorem** - *If a function $K(a,b)$ respects a few mathematical conditions called Mercer's conditions (K must be continous, symmetric in its arguments so $K(a,b) = K(b,a)$, then there exists a function $\\phi$ that maps **a** and **b** into another space, possibly with higher dimensions, such that $K(a, b) = \\phi(a)^T*\\phi(b)$\n",
    "\n",
    "*For linear SVM classifiers, one method is to use Gradient Descent to minimize the cost function which is derived from the primal problem but unfortunately, converges more slowly than the other methods based on QP *\n",
    "\n",
    "**Linear SVM classifier cost function**\n",
    "\n",
    "$$ j(w, b) = \\frac{1}{2}w^T * w +C \\sum_{i= 1}^{m} max(0,1 -t^{(i)}(w^T * x^{(i)} + b)) $$\n",
    "\n",
    "*with $max(0, 1-t)$ being the hinge loss function*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
