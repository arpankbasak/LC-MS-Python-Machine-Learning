{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numerous problems, the measured data vectors are high dimensional and there are reasons to assume that the data lie near a lower dimensional manifold. High dimensional data is regarded are regarded as multiple, indirect measurements of an underlying source which cannot be directly measured. Learning a suitable low dimensional manifold from high dimensional data is essentially the same as learning this underlying source.\n",
    "\n",
    "**Dimensionality reduction**- also known as manifold learning is a process of deriving a set of degrees of freedom which can be used to reproduce most of the variability of the data set.\n",
    "\n",
    "Manifold learning/ Data dimensionality reduction techniques are used primarily for :\n",
    "\n",
    "- Data Dimensionality reduction : Producing a compact low dimensional encoding of a given high dimensional dataset\n",
    "\n",
    "- Data Visualization : providing an interpretation of a given data set in terms of intrinsic degree of freedom, usually as a byproduct of data dimensionality reduction.\n",
    "\n",
    "- Preprocessing for supervised learning : simplifying, reduction and cleaning of data for subsequent machine learning algorithms as this speeds up training processes\n",
    "\n",
    "High dimensional datasets are at risk of being very sparse i.e, most training instances are likely to be far away from each other meaning that any new instance will likely be far away from any training instance, making predictions much less reliable in lower dimensions sicne they will be based on much larger extrapolations. The more dimensions a training set has, the greater the risk of overfitting it. Theoretically speaking, the solution to dimensionality could be to increase the size of the training set to reach a sufficient density of training instances, unfortunately, in practise, the number of training instances required to reach a given density grows exponentially with the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Approaches to Dimensionality reduction are:\n",
    "\n",
    "- **Projection** - involves projecting training instances to a much lower dimensional subspace of the high dimensional subspace.\n",
    "\n",
    "- **Manifold Learning**- is based on the manifold hypothesis that states that high dimensional data tend to lie in the vicinity of a low dimensional manifold or the data generating distribution is assumed to cocnentrate near regions of low dimensionality.Manifold learning seeks a differential algebraic structure in the state space to be inferred from teh sampled data point clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First identifies the hyperplane that lies clost to a  linear subspace of the high dimensional space and then projects the data onto it(the axis that preserves the maximum amount of variance, leading to loss of the least amount of information or the axis that minimizes the the mean squared distance between the original dataset and its projection onto that axis).\n",
    "\n",
    "The principal component of a training set is found through Singular Value Decomposition. Singular Value Decomposition of a matrix is the factorization of A into the product of three matrices $A = UDV^T$ where teh columns of U and V are orthornomal and the matrix D is diagonal with positive real entries.\n",
    "\n",
    "$$ V^T = ( c_1 c_2...  .c_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "#importing important libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as  sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import csv\n",
    "\n",
    "# datafile upload and data preparation\n",
    "df = pd.read_csv(\"diffreport3.csv\", sep= \",\")\n",
    "\n",
    "d1 = df.drop(\"name\", axis = 1)\n",
    "d2 = d1.drop(\"isotopes\", axis = 1)\n",
    "d3 = d2.drop(\"adduct\", axis = 1)\n",
    "d4 = d3.drop(\"tstat\", axis = 1)\n",
    "d5 = d4.drop(\"pvalue\", axis = 1)\n",
    "d6 = d5.drop(\"fold\", axis = 1)\n",
    "d7 = d6.drop(d6.columns[0], axis = 1)\n",
    "d8 = d7.drop(\"npeaks\", axis = 1)\n",
    "d9 = d8.drop(\"Eta6\", axis = 1)\n",
    "d10 = d9.drop(\"Eta8\", axis = 1)\n",
    "columns = ['Eta6_0', 'Eta6_2', 'Eta6_3', 'Eta8.1', 'Eta82', 'Eta83', 'Seq_ID']\n",
    "df1 = pd.DataFrame(d10, columns = columns)\n",
    "\n",
    "#making a smaller dataset for test purposes\n",
    "df1 = df1[0:200]\n",
    "\n",
    "# creation of train and testing sets\n",
    "\n",
    "def get_train_test(df, y_col, x_cols, ratio):\n",
    "\n",
    "    mask = np.random.rand(len(df)) > ratio\n",
    "    df_train = df[mask]\n",
    "    df_test = df[~mask]\n",
    "       \n",
    "    Y_train = df_train[y_col].values\n",
    "    Y_test = df_test[y_col].values\n",
    "    X_train = df_train[x_cols].values\n",
    "    X_test = df_test[x_cols].values\n",
    "    return df_train, df_test, X_train, Y_train, X_test, Y_test\n",
    " \n",
    "y_col = 'Seq_ID'\n",
    "x_cols = list(df1.columns.values)\n",
    "x_cols.remove(y_col)\n",
    " \n",
    "train_test_ratio = 0.7\n",
    "df_train, df_test, X_train, Y_train, X_test, Y_test = get_train_test(df1, y_col, x_cols, train_test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition via Numpy\n",
    "\n",
    "X_centered = X_train -X_train.mean(axis = 0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projecting the training set down to d dimensions\n",
    "\n",
    "$$X_{d-proj} = X.W_d$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = V.T[:, :2]\n",
    "X2D= X_centered.dot(W2)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni4oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f61fd65668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the following code computes PCA without reducing dimensionality and then computes the minimum number of dimensions required to preserve 95% of the training sets variance\n",
    "\n",
    "pca= PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "#plot the number of dimensions \n",
    "\n",
    "plt.plot(cumsum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reconstruction Error**- is the mean squared distance between the original data and the reconstructed data (compressed and then decompressed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 3)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Inverse transformation, back to the original number of dimensions\n",
    "\n",
    "$$X_{recovered} = X_{d_proj} * W_d^T$$\n",
    "\n",
    "Incremental PCA - splits the training set into mini-batches and then sequentially applies PCA, avoiding the need to load the whole data set in memory in order to run the SVD Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches= 100\n",
    "inc_pca = IncrementalPCA(n_components = 2)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "    X_reduced = inc_pca.transform(X_train)\n",
    "    \n",
    "#alternatively, one could use  Numpy's memmap class which allows one to manipulate a large array stored in a binary file\n",
    "\n",
    "X_mm = np.memmap(filename, dtype = \"float32\", mode = \"readonly\", shape = (m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components = 2, batch_size = batch_size)\n",
    "inc_pca.fit(X_mm)\n",
    "\n",
    "# Randomized PCA\n",
    "\n",
    "rnd_pca = PCA(n_components = 3, svd_solver = \"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear decision boundary in the high dimensional feature space corresponds to a complex non-linear decision boundary in the original space. Kernel PCA allows one to make complex non-linear projections for dimensionality reduction and is perfect for preserving clusters of instances after projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel = \"rbf\", gamma = 0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearch to find the best kernel and gamma values\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf= Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components = 2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "            \"kpca__gamma\":np.linspace(0.03, 0.05, 10),\n",
    "            \"kpca__kernel\":[\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf,param_grid, cv = 3)\n",
    "grid_search.fit(X_train,Y_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93408862238397904.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruction pre-image\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel = \"rbf\", gamma = 0.043,\n",
    "                   fit_inverse_transform = True)\n",
    "X_reduced = rbf_pca.fit_transform(X_train)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "#computing reconstruction image\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X_train, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locally Linear Embedding** - is a manifold learning  eigenvector method for non linear dimensionality reduction that computes low dimensional, neighbourhood preserving embeddings of high dimensional inputs.Unlike clustering, LLE maps its inputs into a single global coordinate system of lower dimensionality and its optimizations do not involve local minima.\n",
    "\n",
    "*Step 1: Linearly modelling local relationships*\n",
    "$$ \\hat{W} = argmin_W \\sum\\limits_{i =1} ^{m} \\lvert\\lvert x^{(i)}\\sum\\limits_{j =1} ^{m}\\hat{w}_{i, j} z^{(j)}\\rvert\\rvert^2$$ \n",
    "\n",
    "*Reducing dimensionality while preserving relationships*\n",
    "\n",
    "$$ \\hat{W} = argmin_Z\\sum\\limits_{i =1} ^{m} \\lvert\\lvert z^{(i)} \\sum\\limits_{j =1} ^{m}\\hat{w}_{i, j} z^{(j)}\\rvert\\rvert^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components = 2, n_neighbors = 10)\n",
    "X_reduced = lle.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multidimensional Scaling** - computes embeddings that attempt to preserve pairwise distances or generalized disparities between data points.\n",
    "\n",
    "**Isomap ** - computes embeddings by creating a graph that connects each instance to its nearest neighbours, then reduces dimensionality while trying to preserve geodesic distances between instances.\n",
    "\n",
    "**t-Distributed Stochastic Neighbour Embedding** - encodes  small neighbourhood relationships in the high dimensional space and in the embedding as probability distributions. The embeddings are defined via an iterative minimization of the loss of information when placing the point in the embedding.\n",
    "\n",
    "**Linear Discriminant Analysis** - is a learning algorithm that elucidates the most discriminant axes between classes and uses the axes to define a hyperplane onto which  to project the data, keeping classes as far apart as possible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
